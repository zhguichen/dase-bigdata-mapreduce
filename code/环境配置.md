# Hadoop 环境配置指南

## 初始设置

### 设置 hostname 为 master

```bash
sudo hostnamectl set-hostname master
```

### 验证是否设置成功

```bash
hostnamectl
# 或者
hostname
```

### 更新系统并安装必要工具

```bash
sudo apt update
sudo apt install -y openjdk-8-jdk wget ssh rsync vim
```

### 验证 Java 安装

```bash
java -version
javac -version
```

### 下载 Hadoop 3.4.2（使用阿里云镜像加速）

```bash
cd /opt
sudo wget https://mirrors.aliyun.com/apache/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz
```

### 解压 Hadoop

```bash
sudo tar -xzf hadoop-3.4.2.tar.gz
```

### 重命名目录

```bash
sudo mv hadoop-3.4.2 hadoop
```

### 配置环境变量

编辑 `~/.bashrc` 文件：

```bash
vim ~/.bashrc
```

在文件末尾添加以下内容：

```bash
# ============================================
# Java Environment
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib

# Hadoop Environment
export HADOOP_HOME=/opt/hadoop
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_YARN_HOME=${HADOOP_HOME}
export HADOOP_INSTALL=${HADOOP_HOME}

# PATH
export PATH=${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}
# ============================================
```

### 使环境变量生效

```bash
source ~/.bashrc
```

### 验证环境变量配置

```bash
echo $JAVA_HOME
echo $HADOOP_HOME
echo $PATH
```

### 验证 Hadoop 安装

```bash
hadoop version
```

## 集群节点信息

**集群配置**：4 个节点（1 个 master + 3 个 slave）

| 节点名称 | 内网IP | 公网IP | 角色 |
|---------|--------|--------|------|
| master  | 172.31.12.133 | 47.116.112.198 | NameNode, ResourceManager, DataNode, NodeManager |
| slave1  | 172.31.12.134 | - | DataNode, NodeManager |
| slave2  | 172.31.12.135 | - | DataNode, NodeManager |
| slave3  | 172.31.12.136 | - | DataNode, NodeManager |

**集群节点数量**：

- HDFS DataNode：4 个（master、slave1、slave2、slave3）
- YARN NodeManager：4 个（master、slave1、slave2、slave3）
- HDFS 副本数：3
- HDFS 块大小：64MB

需要配置 SSH 免密登录，让所有节点之间可以互相访问。

## 步骤1：在 master 节点配置 SSH 密钥

```bash
# 生成 SSH 密钥对（如果还没有）
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

# 将公钥添加到 authorized_keys（用于本地免密登录）
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# 设置权限
chmod 600 ~/.ssh/authorized_keys
chmod 700 ~/.ssh

# 测试本地 SSH 连接
ssh localhost
```

## 步骤2：复制 master 镜像生成三个子节点

在云平台复制 master 镜像，生成 slave1, slave2, slave3 三个节点。

**注意**：复制后的节点会有和 master 相同的 SSH 密钥对。

## 步骤3：配置各节点的 hostname 和 hosts 文件

### 在 master 节点执行

```bash
sudo hostnamectl set-hostname master
sudo vim /etc/hosts
```

添加以下内容（根据实际IP修改）：

```
172.31.12.133 master
172.31.12.134 slave1
172.31.12.135 slave2
172.31.12.136 slave3
```

### 在 slave1 节点执行

```bash
sudo hostnamectl set-hostname slave1
sudo vim /etc/hosts
```

添加相同的内容（所有节点的 `/etc/hosts` 应该一致）

### 在 slave2 节点执行

```bash
sudo hostnamectl set-hostname slave2
sudo vim /etc/hosts
```

添加相同的内容

### 在 slave3 节点执行

```bash
sudo hostnamectl set-hostname slave3
sudo vim /etc/hosts
```

添加相同的内容

## 步骤4：配置所有节点的 SSH 免密登录

由于所有节点都是从 master 镜像复制的，它们拥有相同的 SSH 密钥对。因此，只需要将公钥添加到所有节点的 `authorized_keys` 即可。

**方法1：手动方式（推荐用于理解原理）**

在每个节点（master, slave1, slave2, slave3）上执行：

```bash
# 1. 确保 ~/.ssh 目录存在
mkdir -p ~/.ssh
chmod 700 ~/.ssh

# 2. 将公钥添加到 authorized_keys（如果还没有）
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# 3. 设置权限
chmod 600 ~/.ssh/authorized_keys
```

## 步骤5：验证 SSH 免密登录

在 master 节点测试到所有节点的连接：

```bash
ssh master "echo 'master -> master: OK'"
ssh slave1 "echo 'master -> slave1: OK'"
ssh slave2 "echo 'master -> slave2: OK'"
ssh slave3 "echo 'master -> slave3: OK'"
```

在其他节点也可以测试：

```bash
# 在 slave1 节点：
# ssh master "echo 'slave1 -> master: OK'"
# ssh slave2 "echo 'slave1 -> slave2: OK'"
# ssh slave3 "echo 'slave1 -> slave3: OK'"
```

---

## Hadoop 集群配置

所有配置文件位于 `/opt/hadoop/etc/hadoop/` 目录下。

### 1. 配置 hadoop-env.sh

编辑 hadoop-env.sh 文件，设置 JAVA_HOME：

```bash
sudo vim /opt/hadoop/etc/hadoop/hadoop-env.sh
```

找到 `export JAVA_HOME` 这一行，修改为：

```bash
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
```

### 2. 配置 core-site.xml

编辑 core-site.xml 文件：

```bash
sudo vim /opt/hadoop/etc/hadoop/core-site.xml
```

将内容修改为：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- 指定 HDFS 的 NameNode 地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
    
    <!-- 指定 Hadoop 临时目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/hadoop/tmp</value>
    </property>
</configuration>
```

### 3. 配置 hdfs-site.xml

编辑 hdfs-site.xml 文件：

```bash
sudo vim /opt/hadoop/etc/hadoop/hdfs-site.xml
```

将内容修改为：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- 设置 HDFS 副本数量 -->
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    
    <!-- 设置 HDFS 块大小为 64MB -->
    <property>
        <name>dfs.blocksize</name>
        <value>67108864</value>
        <description>HDFS block size: 64MB (64 * 1024 * 1024 bytes)</description>
    </property>
</configuration>
```

### 4. 配置 mapred-site.xml

编辑 mapred-site.xml 文件：

```bash
sudo vim /opt/hadoop/etc/hadoop/mapred-site.xml
```

将内容修改为：

**配置说明**：

- Map/Reduce Container 内存设置为 2GB，适合 4c8G 机器（每节点最多同时运行 3 个 container）
- JVM 堆内存设置为 1536MB，略小于 Container 内存，为堆外操作留出空间
- Map 推测执行开启，可以加速慢任务
- Reduce 推测执行关闭，避免重复计算，便于实验观察

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- 指定 MapReduce 运行在 YARN 上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    
    <!-- MapReduce 应用程序的 classpath -->
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
    
    <!-- Reduce 启动策略参数（本次实验的关键参数）-->
    <property>
        <name>mapreduce.job.reduce.slowstart.completedmaps</name>
        <value>0.05</value>
        <description>当 Map 任务完成的比例达到此值时开始启动 Reduce 任务</description>
    </property>
    
    <!-- YARN ApplicationMaster 环境变量 -->
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
    </property>
    
    <!-- Map 任务环境变量 -->
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
    </property>
    
    <!-- Reduce 任务环境变量 -->
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
    </property>
    
    <!-- MapReduce JobHistory Server 配置 -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master:10020</value>
        <description>MapReduce JobHistory Server IPC地址</description>
    </property>
    
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master:19888</value>
        <description>MapReduce JobHistory Server Web UI地址</description>
    </property>
    
    <property>
        <name>mapreduce.jobhistory.intermediate-done-dir</name>
        <value>/mr-history/tmp</value>
        <description>作业历史文件的临时目录</description>
    </property>
    
    <property>
        <name>mapreduce.jobhistory.done-dir</name>
        <value>/mr-history/done</value>
        <description>作业历史文件的永久存储目录</description>
    </property>
    
    <!-- Map/Reduce Container 内存配置 -->
    <!-- Map Container 内存：2G -->
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>2048</value>
        <description>Map Container 内存分配，2GB</description>
    </property>
    
    <!-- Reduce Container 内存：2G -->
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>2048</value>
        <description>Reduce Container 内存分配，2GB</description>
    </property>
    
    <!-- Map JVM 堆内存 -->
    <property>
        <name>mapreduce.map.java.opts</name>
        <value>-Xmx1536m</value>
        <description>Map 任务 JVM 堆内存，略小于 Container 内存以留出堆外空间</description>
    </property>
    
    <!-- Reduce JVM 堆内存 -->
    <property>
        <name>mapreduce.reduce.java.opts</name>
        <value>-Xmx1536m</value>
        <description>Reduce 任务 JVM 堆内存，略小于 Container 内存以留出堆外空间</description>
    </property>
    
    <!-- 推测执行配置 -->
    <!-- Map 推测执行：开启 -->
    <property>
        <name>mapreduce.map.speculative</name>
        <value>true</value>
        <description>开启 Map 任务的推测执行，加速慢任务</description>
    </property>
    
    <!-- Reduce 推测执行：关闭 -->
    <property>
        <name>mapreduce.reduce.speculative</name>
        <value>false</value>
        <description>关闭 Reduce 任务的推测执行，避免重复计算，便于实验观察</description>
    </property>
</configuration>
```

### 5. 配置 yarn-site.xml

编辑 yarn-site.xml 文件：

```bash
sudo vim /opt/hadoop/etc/hadoop/yarn-site.xml
```

将内容修改为：

**配置说明**：

- 每节点分配 6GB 内存给 YARN，预留 2GB 给系统和 Hadoop 服务（适合 4c8G 机器）
- 每节点分配 3 个 vcores，限制并发 container 数量，避免过载
- Container 最小/最大内存分配设置为 1GB/6GB，与节点资源匹配

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- 指定 ResourceManager 的地址 -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    
    <!-- NodeManager 辅助服务，用于 MapReduce Shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    
    <!-- YARN 应用程序的 classpath -->
    <property>
        <name>yarn.application.classpath</name>
        <value>$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,$HADOOP_COMMON_HOME/share/hadoop/common/lib/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,$HADOOP_YARN_HOME/share/hadoop/yarn/*,$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*</value>
    </property>
    
    <!-- 日志聚合配置（用于JobHistory Server）-->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
        <description>启用日志聚合功能</description>
    </property>
    
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
        <description>聚合日志保留7天（604800秒）</description>
    </property>
    
    <property>
        <name>yarn.log.server.url</name>
        <value>http://master:19888/jobhistory/logs</value>
        <description>JobHistory Server 日志URL</description>
    </property>
    
    <!-- YARN 节点资源配置 -->
    <!-- 每个 NodeManager 能给 YARN 用的内存：6G -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>6144</value>
        <description>每个 NodeManager 能给 YARN 用的内存，4c8G 机器预留 2G 给系统和 Hadoop</description>
    </property>
    
    <!-- 每个 NodeManager 能给 YARN 用的 CPU 核数：3 核 -->
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>3</value>
        <description>每个 NodeManager 能给 YARN 用的 CPU 核数，限制并发 container 数量</description>
    </property>
    
    <!-- Container 最小内存分配 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
        <description>Container 最小内存分配，1GB</description>
    </property>
    
    <!-- Container 最大内存分配 -->
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>6144</value>
        <description>Container 最大内存分配，等于节点总内存</description>
    </property>
</configuration>
```

### 6. 配置 workers 文件

编辑 workers 文件（替换旧版本的 slaves 文件）：

```bash
sudo vim /opt/hadoop/etc/hadoop/workers
```

将内容修改为（列出所有 DataNode 和 NodeManager 节点）：

```
master
slave1
slave2
slave3
```

**注意**：本配置中 master 节点也参与数据存储和计算，因此包含在 workers 文件中。这样 master 节点会同时运行 DataNode 和 NodeManager 服务，提高集群的计算和存储能力。

### 7. 创建必要的目录

在 master 节点创建 Hadoop 所需的目录：

```bash
sudo mkdir -p /opt/hadoop/tmp
sudo chown -R root:root /opt/hadoop
```

### 8. 将配置文件分发到所有节点

使用 scp 或 rsync 将配置好的 Hadoop 目录分发到所有从节点：

**方法 1：使用 scp（逐个复制）**

```bash
# 分发到 slave1
sudo scp -r /opt/hadoop/etc/hadoop/* slave1:/opt/hadoop/etc/hadoop/

# 分发到 slave2
sudo scp -r /opt/hadoop/etc/hadoop/* slave2:/opt/hadoop/etc/hadoop/

# 分发到 slave3
sudo scp -r /opt/hadoop/etc/hadoop/* slave3:/opt/hadoop/etc/hadoop/
```

**方法 2：使用循环批量分发（推荐）**

```bash
for node in slave1 slave2 slave3; do
    echo "==== 正在分发配置到 $node ===="
    sudo scp -r /opt/hadoop/etc/hadoop/* $node:/opt/hadoop/etc/hadoop/
done
```

**方法 3：使用 rsync（更高效）**

```bash
for node in slave1 slave2 slave3; do
    echo "==== 正在同步配置到 $node ===="
    sudo rsync -avz /opt/hadoop/etc/hadoop/ $node:/opt/hadoop/etc/hadoop/
done
```

### 9. 在所有从节点创建必要的目录

在 master 节点批量执行：

```bash
for node in slave1 slave2 slave3; do
    echo "==== 在 $node 上创建目录 ===="
    ssh $node "sudo mkdir -p /opt/hadoop/tmp && sudo chown -R root:root /opt/hadoop"
done
```

### 10. 格式化 HDFS NameNode

**重要**：仅在第一次启动集群前执行一次，后续不要重复执行！

在 master 节点执行：

```bash
hdfs namenode -format
```

看到 "Storage directory ... has been successfully formatted" 表示格式化成功。

### 11. 创建 JobHistory Server 所需的 HDFS 目录

在启动集群前，先启动 HDFS，然后创建 JobHistory Server 需要的目录：

```bash
# 启动 HDFS
start-dfs.sh

# 创建历史目录
hdfs dfs -mkdir -p /mr-history/tmp /mr-history/done
hdfs dfs -chmod -R 777 /mr-history

# 验证目录创建成功
hdfs dfs -ls -R /mr-history
```

### 12. 启动 YARN 和 JobHistory Server

在 master 节点执行：

```bash
# 启动 YARN（ResourceManager 和所有 NodeManager）
start-yarn.sh

# 启动 MapReduce JobHistory Server
mapred --daemon start historyserver
```

### 13. 验证集群状态

#### 13.1 使用 jps 命令查看进程

**在 master 节点：**

```bash
jps
```

应该看到：NameNode、SecondaryNameNode、ResourceManager、JobHistoryServer、DataNode、NodeManager

**在 slave 节点：**

```bash
jps
```

应该看到：DataNode、NodeManager

#### 13.2 使用命令查看集群状态

```bash
# 查看 HDFS 集群报告
hdfs dfsadmin -report
# 输出中会显示 "Live datanodes (4):" 表示有4个DataNode节点
# 节点列表：master, slave1, slave2, slave3

# 查看 YARN 节点列表
yarn node -list
# 输出中会显示 "Total Nodes:4" 表示有4个NodeManager节点
# 节点列表：master, slave1, slave2, slave3

# 验证 HDFS 块大小配置
hdfs getconf -confKey dfs.blocksize
# 应该输出：67108864 (64MB)

# 创建实验目录
hdfs dfs -mkdir -p /user/root
hdfs dfs -mkdir -p /input
hdfs dfs -mkdir -p /output
```

**当前集群配置**：

- **集群节点总数**：4 个节点（1 个 master + 3 个 slave）
- **HDFS DataNode 数量**：4 个（master、slave1、slave2、slave3）
- **YARN NodeManager 数量**：4 个（master、slave1、slave2、slave3）
- **HDFS 副本数**：3（配置在 hdfs-site.xml 中）
- **HDFS 块大小**：64MB（配置在 hdfs-site.xml 中）
- **YARN 节点资源**：每节点 6GB 内存、3 个 vcores（配置在 yarn-site.xml 中）
- **Map/Reduce 内存**：Map 和 Reduce 各 2GB，JVM 堆内存 1536MB（配置在 mapred-site.xml 中）
- **推测执行**：Map 开启，Reduce 关闭（配置在 mapred-site.xml 中）

#### 13.3 通过 Web UI 查看

- **HDFS NameNode**：http://47.116.112.198:9870
- **YARN ResourceManager**：http://47.116.112.198:8088
- **JobHistory Server**：http://47.116.112.198:19888

**注意**：确保阿里云安全组已开放端口 9870、8088、19888。

**重要**：如果从本地浏览器访问，需要在本地电脑的 hosts 文件中添加：

```
47.116.112.198 master
```

- Windows: `C:\Windows\System32\drivers\etc\hosts`
- macOS/Linux: `/etc/hosts`

这样才能正常访问 YARN Web UI 中的应用详情页面（Tracking UI）。

### 14. 停止 Hadoop 集群（需要时）

```bash
# 停止 JobHistory Server
mapred --daemon stop historyserver

# 停止 YARN
stop-yarn.sh

# 停止 HDFS
stop-dfs.sh
```

### 15. 常见问题排查

#### 问题1：DataNode 无法启动

检查日志：`/opt/hadoop/logs/hadoop-root-datanode-*.log`

如果 clusterID 不匹配，清理数据并重新格式化：

```bash
# 在所有节点清理数据
for node in master slave1 slave2 slave3; do
    ssh $node "sudo rm -rf /opt/hadoop/tmp/*"
done

# 重新格式化 NameNode
hdfs namenode -format

# 重新启动集群
start-dfs.sh
start-yarn.sh
mapred --daemon start historyserver
```

#### 问题2：Web UI 中点击 Tracking UI 显示无法找到应用

**原因**：本地浏览器无法解析 `master` 主机名，或 JobHistory Server 未启动。

**解决方法**：

1. 在本地电脑的 hosts 文件中添加：

   ```
   47.116.112.198 master
   ```

2. 确保 JobHistory Server 已启动：

   ```bash
   jps | grep JobHistoryServer
   ```
   
   如果没有运行，启动它：

   ```bash
   mapred --daemon start historyserver
   ```

3. 验证 JobHistory Server Web UI 可访问：

   ```bash
   curl -I http://master:19888
   ```

---

## 清单

- [ ] 所有节点 SSH 免密登录配置完成
- [ ] 所有节点 Hadoop 环境变量配置完成
- [ ] 修改 5 个配置文件（hadoop-env.sh, core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml）
- [ ] 配置 workers 文件
- [ ] 将配置分发到所有 slave 节点
- [ ] 格式化 NameNode（仅首次）
- [ ] 启动 HDFS 并创建 /mr-history 目录
- [ ] 启动 YARN 和 JobHistory Server
- [ ] 验证：jps 查看进程（包括 JobHistoryServer），hdfs dfsadmin -report 查看集群，yarn node -list 查看节点
- [ ] 本地电脑 hosts 文件已配置（47.116.112.198 master）
- [ ] 阿里云安全组已开放端口：9870（HDFS）、8088（YARN）、19888（JobHistory）
